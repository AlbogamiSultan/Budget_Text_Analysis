{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "\n",
    "\n",
    "1. **Obtaining the data -** data consists of budget text documents in the form of PDF files obtained from the following organizations: \n",
    "\n",
    "   * [Guilford County](https://www.guilfordcountync.gov/home/showdocument?id=9497)\n",
    "   * [Durham County](https://www.dconc.gov/home/showdocument?id=27985)\n",
    "   * [City of Durham](https://durhamnc.gov/DocumentCenter/View/27412/FY20-Final-Budget)\n",
    "   * [City of Charlotte](https://charlottenc.gov/budget/FY2020%20Documents/FY%202020%20Adopted%20Budget%20Book%207-31%20Complete.pdf)\n",
    "   * [Mecklenburg County](https://www.mecknc.gov/CountyManagersOffice/OMB/Documents/FY2020%20Adopted%20Budget.pdf) <br/>\n",
    "   * [Wake County](http://www.wakegov.com/budget/fy20/Documents/FY20%20Adopted%20Budget%20Book.pdf)\n",
    "   * [City of Raleigh](https://user-2081353526.cld.bz/FY2020AdoptedBudget)\n",
    "   \n",
    "After the PDF files are collected, they are compressed to reduce the size getting them ready to be converted into CSV files using an app developed by project mentor:\n",
    "           **[Jason Jones](https://www.linkedin.com/in/jones-jason-adam/),**\n",
    "           **click [here](https://jason-jones.shinyapps.io/Emotionizer/) for the App**\n",
    "       \n",
    "2. **Cleaning the data -** performing some popular text pre-processing techniques\n",
    "\n",
    "\n",
    "3. **Organizing the data -** organizing the cleaned data into a way that is easy to input into other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FY2013 Data Preprocessig Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change the current directory to read the data\n",
    "os.chdir(r\"C:\\Users\\Sultan\\Documents\\GitHub\\Budget_Text_Analysis\\util\\data\\FY2013\\structured\\original\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Obtaining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and labling data for all organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1- Reading Guilford-County data file \n",
    "GC_df = pd.read_csv(\"GuilfordCountyOriginalDataFY13.csv\", engine='python')\n",
    "# inserting \"organization\" column with static value \n",
    "# corresponding to the organization in question \n",
    "GC_df.insert(2, \"organization\", \"Guilford County\")\n",
    "\n",
    "\n",
    "# 2- For Charlotte-City data\n",
    "CC_df = pd.read_csv(r'CharlotteCityOriginalDataFY13.csv', engine='python')\n",
    "CC_df.insert(2, \"organization\", \"Charlotte City\")\n",
    "\n",
    "# 3- For Durham-City data\n",
    "DCity_df = pd.read_csv(r'DurhamCityOriginalDataFY13.csv', engine='python')\n",
    "DCity_df.insert(2, \"organization\", \"Durham City\")\n",
    "\n",
    "# 4- For Durham-County data\n",
    "DCounty_df = pd.read_csv(r'DurhamCountyOriginalDataFY13.csv', engine='python')\n",
    "DCounty_df.insert(2, \"organization\", \"Durham County\")\n",
    "\n",
    "# 5- For Mecklenburg-County data\n",
    "MC_df = pd.read_csv(r'MecklenburgCountyOriginalDataFY13.csv', engine='python')\n",
    "MC_df.insert(2, \"organization\", \"Mecklenburg County\")\n",
    "\n",
    "# 6- For Raleigh-City data\n",
    "RC_df = pd.read_csv(r'RaleighCityOriginalDataFY13.csv', engine='python')\n",
    "RC_df.insert(2, \"organization\", \"Raleigh City\")\n",
    "\n",
    "# 7- For Wake-County data\n",
    "WC_df = pd.read_csv(r'WakeCountyOriginalDataFY13.csv', engine='python')\n",
    "WC_df.insert(2, \"organization\", \"Wake County\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine all dataframes into a single dataframe using concat() function\n",
    "# Row lables are adjusted automaticlly by passing ignore_index=True\n",
    "data =  pd.concat([GC_df, CC_df, DCity_df, \n",
    "                   DCounty_df, MC_df, RC_df, WC_df], ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# listing columns in data frame \n",
    "list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping and reordering columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete columns using the columns parameter of drop\n",
    "data = data.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "# re-order columns\n",
    "data = data[['page_number','word','organization']]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding \"Year\" column with a static value corresponding to the year in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.insert(3, \"year\", \"FY2013\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Text normalization:\n",
    "\n",
    "* ##### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using a function to lowercase all text entries in column 'word'\n",
    "data['word'] = data['word'].apply(lambda x: \" \".join(x.lower() \n",
    "                                                     for x in x.split()))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the set of stop words the first time\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "data['word'] = data['word'].apply(lambda x: \" \".join(x \n",
    "                            for x in x.split() if x not in stop_words))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex with the pattern replacing not alphanumeric/whitespace with a space\n",
    "data['word'] = data['word'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing empty cells with null values, so we can drop all NaN values later\n",
    "data['word'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# Drop all NaN values from the data frames\n",
    "data.dropna(subset=['word'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Organizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already created a corpus in an earlier step. The definition of a corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at our dataframe\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe to one single and clean csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export dataframe to csv\n",
    "data.to_csv(r\"C:\\Users\\Sultan\\Documents\\GitHub\\Budget_Text_Analysis\\util\\data\\PreprocessedOriginalData\\PreprocessedOriginalDataFY13.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change the dirctory for pickle file to be stored properly\n",
    "os.chdir(r\"C:\\Users\\Sultan\\Documents\\GitHub\\Budget_Text_Analysis\\util\\data\\PreprocessedOriginalData\\pickle\") \n",
    "# Let's pickle it for later use\n",
    "data.to_pickle(\"DataFY13.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
